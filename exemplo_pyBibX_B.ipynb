{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO7ZYzeden9c"
      },
      "outputs": [],
      "source": [
        "# Restart the session afther this cell to avoid Google Colab errors\n",
        "!pip install --upgrade --force-reinstall numpy==1.26.4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF29icAhigEy"
      },
      "outputs": [],
      "source": [
        "!pip install pybibx\n",
        "!pip install tabulate tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGlz58a_iQhX"
      },
      "outputs": [],
      "source": [
        "# Dowload .bib file\n",
        "#!wget https://github.com/Valdecy/pyBibX/raw/main/assets/bibs/scopus.bib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUDAb0D_aV53"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import textwrap\n",
        "\n",
        "from pybibx.base import pbx_probe\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U94aI4NIjY-k"
      },
      "outputs": [],
      "source": [
        "# Load .bib\n",
        "# Arguments: file_bib = 'filename.bib'; db = 'scopus', 'wos', 'pubmed'; del_duplicated = True, False\n",
        "file_name = 'dados/scopus.bib'\n",
        "database  = 'scopus'\n",
        "bibfile   = pbx_probe(file_bib = file_name, db = database, del_duplicated = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(bibfile.data.document_type.value_counts())\n",
        "filtro = ['Article','Conference paper']\n",
        "bibfile.data = bibfile.data[bibfile.data['document_type'].isin(filtro)]\n",
        "print(bibfile.data.document_type.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_name_acm = 'dados/acm.bib'\n",
        "database_acm  = 'acm'\n",
        "bibfile_acm   = pbx_probe(file_bib = file_name_acm, db = database_acm, del_duplicated = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bibfile.merge_database(file_bib=file_name_acm, db=database_acm, del_duplicated=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bja7UImUpuK"
      },
      "outputs": [],
      "source": [
        "# Health Analysis\n",
        "health = bibfile.health_bib()\n",
        "\n",
        "# Check Health\n",
        "health"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDwUW-yofYlI"
      },
      "outputs": [],
      "source": [
        "print(bibfile.data['abstract'].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjFFdFz3gjX4"
      },
      "outputs": [],
      "source": [
        "!pip install pybtex\n",
        "!pip install bibtexparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXTHbzlJhHcg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import bibtexparser\n",
        "from bibtexparser.bwriter import BibTexWriter\n",
        "from bibtexparser.bibdatabase import BibDatabase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import random\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from functools import partial\n",
        "import pandas as pd\n",
        "import ollama\n",
        "\n",
        "# === CONFIGURAÇÕES GLOBAIS ===\n",
        "MAX_REQUESTS_PER_MINUTE = 300\n",
        "SECONDS_BETWEEN_REQUESTS = 60 / MAX_REQUESTS_PER_MINUTE\n",
        "BATCH_SIZE = 1\n",
        "WORKERS = min(8, cpu_count())\n",
        "MODELS = [\"llama3:8b\", \"gemma3:27b-it-qat\", \"phi4-mini\", \"phi4\"] # \"llama3:8b\", \"gemma3:27b-it-qat\", \"cogito:8b\", \"phi4\"\n",
        "TEMPERATURE = 0\n",
        "\n",
        "QUERY = (\n",
        "    #\"Does this abstract discuss artificial intelligence in feedback for learning management systems or online learning environment on education?\"\n",
        "    \"Analyze the following scientific article abstract and determine whether it \"\n",
        "     \"addresses the use of artificial intelligence to provide feedback in virtual learning environments.\\n\"\n",
        "     \"Consider aspects such as: the application of AI techniques, automated feedback systems, \"\n",
        "     \"digital educational platforms, and online learning. Respond only with ‘Yes’ if the article is related, \"\n",
        "     \"or ‘No’ if it is not.\\n\\n\"\n",
        ")\n",
        "\n",
        "# === UTILITÁRIOS ===\n",
        "def log(text: str, log_path: str) -> None:\n",
        "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"{timestamp} - {text}\\n\")\n",
        "    print(f\"{timestamp} - {text}\")\n",
        "\n",
        "\n",
        "def chunk_dataframe(df, batch_size: int):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i : i + batch_size], i\n",
        "\n",
        "\n",
        "# === CHAMADA AO LLM LOCAL ===\n",
        "def call_local_llm(messages, model: str, temperature: float):\n",
        "    response = ollama.chat(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        options={\"temperature\": temperature},\n",
        "        stream=False\n",
        "    )\n",
        "    return response.message.content.strip()\n",
        "\n",
        "\n",
        "# === PROCESSAMENTO DE LOTE COM RETRIES ===\n",
        "def process_batch_with_retry(\n",
        "    batch_df, global_index, query, model, temperature,\n",
        "    seconds_between_requests, log_path, retry_limit=5\n",
        "):\n",
        "    retry_count = 0\n",
        "    delay = seconds_between_requests + random.uniform(0, 5)\n",
        "\n",
        "    while retry_count < retry_limit:\n",
        "        try:\n",
        "            if model == \"cogito:8b\":\n",
        "                time.sleep(delay)\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": (\n",
        "                        \"Enable deep thinking subroutine.\"\n",
        "                    )}\n",
        "                ]\n",
        "                prompt = f\"{query}\\n\\nYou are a research assistant who helps analyze scientific articles. Restrict yourself to answering the question with exclusively 'yes' or 'no'.\\n\\n\"\n",
        "            else:\n",
        "                time.sleep(delay)\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": (\n",
        "                        \"You are a research assistant who helps analyze scientific articles.\"\n",
        "                    )}\n",
        "                ]\n",
        "                prompt = f\"{query}\\n\\nRestrict yourself to answering the question with exclusively 'yes' or 'no'.\\n\\n\"\n",
        "\n",
        "            for i, row in batch_df.iterrows():\n",
        "                prompt += f\"Abstract {i + 1}:\\n{row['abstract']}\\n\\n\"\n",
        "\n",
        "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "            content = call_local_llm(messages, model=model, temperature=temperature)\n",
        "            answers = content.splitlines()\n",
        "\n",
        "            results = []\n",
        "            coluna = \"relevant_\" + model.split(\":\")[0]\n",
        "            for answer, (_, row) in zip(answers, batch_df.iterrows()):\n",
        "                result = row.to_dict()\n",
        "                clean = answer.strip().lower()\n",
        "                result[coluna] = (clean == \"yes\")\n",
        "                results.append(result)\n",
        "\n",
        "            log(f\"[{model}] Lote {global_index} OK\", log_path)\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            retry_count += 1\n",
        "            wait_time = 2 ** retry_count + random.uniform(0, 1)\n",
        "            log(f\"[{model}][ERRO] Lote {global_index}, tentativa {retry_count}: {e}\", log_path)\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "    log(f\"[{model}][FALHA] Lote {global_index} excedeu tentativas\", log_path)\n",
        "    return []\n",
        "\n",
        "\n",
        "def process_args_wrapper(args, query, model, temperature, seconds_between_requests, log_path):\n",
        "    return process_batch_with_retry(*args, query=query, model=model,\n",
        "                                    temperature=temperature,\n",
        "                                    seconds_between_requests=seconds_between_requests,\n",
        "                                    log_path=log_path)\n",
        "\n",
        "\n",
        "# === PIPELINE PARA UM MODELO ===\n",
        "def analyze_abstracts_parallel(\n",
        "    df: pd.DataFrame,\n",
        "    query: str,\n",
        "    model: str,\n",
        "    batch_size: int,\n",
        "    workers: int,\n",
        "    result_csv_path: str,\n",
        "    log_path: str,\n",
        "    temperature: float,\n",
        "    seconds_between_requests: float,\n",
        ") -> pd.DataFrame:\n",
        "    if os.path.exists(result_csv_path):\n",
        "        acumulado = pd.read_csv(result_csv_path)\n",
        "        start = len(acumulado)\n",
        "        log(f\"[{model}] Retomando do índice {start}\", log_path)\n",
        "    else:\n",
        "        acumulado = pd.DataFrame()\n",
        "        start = 0\n",
        "\n",
        "    to_process = df.iloc[start:].reset_index(drop=True)\n",
        "    batches = [\n",
        "        (batch, idx + start)\n",
        "        for batch, idx in chunk_dataframe(to_process, batch_size)\n",
        "    ]\n",
        "\n",
        "    log(f\"[{model}] Iniciando {len(batches)} lotes com {workers} workers\", log_path)\n",
        "\n",
        "    with Pool(processes=workers) as pool:\n",
        "        processor = partial(\n",
        "            process_args_wrapper,\n",
        "            query=query,\n",
        "            model=model,\n",
        "            temperature=temperature,\n",
        "            seconds_between_requests=seconds_between_requests,\n",
        "            log_path=log_path\n",
        "        )\n",
        "        for outcome in pool.imap_unordered(processor, batches):\n",
        "            if outcome:\n",
        "                df_part = pd.DataFrame(outcome)\n",
        "                acumulado = pd.concat([acumulado, df_part], ignore_index=True)\n",
        "                acumulado.to_csv(result_csv_path, index=False)\n",
        "\n",
        "    log(f\"[{model}] Processamento completo.\", log_path)\n",
        "    return acumulado\n",
        "\n",
        "\n",
        "# === RUN_ALL_MODELS MODIFICADA ===\n",
        "def run_all_models(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Executa o pipeline para todos os modelos em MODELS e retorna um DataFrame combinado\n",
        "    contendo todas as colunas relevant_<model>.\n",
        "    \"\"\"\n",
        "    combined = df.copy()\n",
        "\n",
        "    for model in MODELS:\n",
        "        model_name = model.split(\":\")[0]\n",
        "        result_path = f\"temp_files/resultados_parciais_{model_name}.csv\"\n",
        "        log_path = f\"temp_files/log_execucao_{model_name}.txt\"\n",
        "\n",
        "        if model == \"gemma3:27b-it-qat\":\n",
        "            resultados = analyze_abstracts_parallel(\n",
        "            df=df,\n",
        "            query=QUERY,\n",
        "            model=model,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            workers=(WORKERS + 1),\n",
        "            result_csv_path=result_path,\n",
        "            log_path=log_path,\n",
        "            temperature=TEMPERATURE,\n",
        "            seconds_between_requests=SECONDS_BETWEEN_REQUESTS\n",
        "        )\n",
        "\n",
        "        elif model == \"phi4-mini\":\n",
        "            resultados = analyze_abstracts_parallel(\n",
        "            df=df,\n",
        "            query=QUERY,\n",
        "            model=model,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            workers=(WORKERS + 4),\n",
        "            result_csv_path=result_path,\n",
        "            log_path=log_path,\n",
        "            temperature=TEMPERATURE,\n",
        "            seconds_between_requests=SECONDS_BETWEEN_REQUESTS\n",
        "        )\n",
        "        elif model == \"llama3\":\n",
        "            resultados = analyze_abstracts_parallel(\n",
        "            df=df,\n",
        "            query=QUERY,\n",
        "            model=model,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            workers=(WORKERS + 4),\n",
        "            result_csv_path=result_path,\n",
        "            log_path=log_path,\n",
        "            temperature=TEMPERATURE,\n",
        "            seconds_between_requests=SECONDS_BETWEEN_REQUESTS\n",
        "        ) \n",
        "        else:\n",
        "            resultados = analyze_abstracts_parallel(\n",
        "            df=df,\n",
        "            query=QUERY,\n",
        "            model=model,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            workers=WORKERS,\n",
        "            result_csv_path=result_path,\n",
        "            log_path=log_path,\n",
        "            temperature=TEMPERATURE,\n",
        "            seconds_between_requests=SECONDS_BETWEEN_REQUESTS\n",
        "        )\n",
        "\n",
        "        col = f\"relevant_{model_name}\"\n",
        "        combined = combined.merge(\n",
        "            resultados[[col]],\n",
        "            left_index=True, right_index=True\n",
        "        )\n",
        "\n",
        "    return combined\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_filter_bases(directory: str) -> pd.DataFrame:\n",
        "    print(f\"Carregando bases de '{directory}'\")\n",
        "    dfs = []\n",
        "    for fname in sorted(os.listdir(directory)):\n",
        "        if fname.lower().endswith('.csv'):\n",
        "            path = os.path.join(directory, fname)\n",
        "            try:\n",
        "                df = pd.read_csv(path)\n",
        "                print(f\"{fname}: {len(df)} registros\")\n",
        "                dfs.append(df)\n",
        "            except Exception as e:\n",
        "                print(f\"Erro lendo {fname}: {e}\")\n",
        "    if not dfs:\n",
        "        return pd.DataFrame()\n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dados = bibfile.data\n",
        "df_ieee = load_and_filter_bases(\"dados\")\n",
        "colunas_desejadas_ieee = ['Document Title', 'Abstract', 'Author Affiliations', 'Authors', 'DOI', 'ISBNs',\n",
        "                             'ISSN', 'Publication Title', 'Publication Year']\n",
        "df_ieee = df_ieee[colunas_desejadas_ieee].copy()\n",
        "#print(df_ieee.columns)\n",
        "colunas_desejadas_scopus = ['title', 'abstract', 'journal', \n",
        "                            'affiliation', 'author', 'doi', 'isbn',\n",
        "                             'issn', 'year']\n",
        "\n",
        "df_scopus = dados[colunas_desejadas_scopus].copy()\n",
        "df_scopus = df_scopus.rename(columns={\n",
        "    'title': 'Document Title',\n",
        "    'abstract': 'Abstract',\n",
        "    'abbrev_source_title': 'Publication Title',\n",
        "    'affiliation': 'Author Affiliations',\n",
        "    'author': 'Authors',\n",
        "    'doi': 'DOI',\n",
        "    'isbn': 'ISBNs',\n",
        "    'issn': 'ISSN',\n",
        "    'journal': 'Publication Title',\n",
        "    'references': 'References',\n",
        "    'url': 'URL',\n",
        "    'year': 'Publication Year'\n",
        "})\n",
        "\n",
        "\n",
        "# Certifique-se de que o DataFrame `dados` contém pelo menos as colunas 'abstract' e outras desejadas\n",
        "#resultados = analyze_abstracts_parallel(dados, query=query_global, model=model, batch_size=5, workers=8)\n",
        "\n",
        "print(df_ieee.shape, '\\t', df_scopus.shape)\n",
        "dados = pd.concat([df_ieee, df_scopus], ignore_index=True)\n",
        "print('Antes da remoção de duplicados: ', dados.shape)\n",
        "dados.columns = dados.columns.str.lower()\n",
        "dados = dados.dropna(subset=['abstract'])\n",
        "dados = dados.drop_duplicates(subset=['abstract'])\n",
        "dados = dados.reset_index(drop=True)\n",
        "print('Após remoção de duplicados: ', dados.shape)\n",
        "\n",
        "resultados = run_all_models(dados)\n",
        "\n",
        "# Salvar CSV final (opcional)\n",
        "resultados.to_csv(\"temp_files/resultados_finais.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT68x1YipfZg"
      },
      "outputs": [],
      "source": [
        "print(resultados.columns)\n",
        "# Exibir os resultados\n",
        "for model in MODELS:\n",
        "    model_name = model.split(\":\")[0]\n",
        "    print(f\"\\nResultados para o modelo {model_name}:\")\n",
        "    print(resultados[f'relevant_{model_name}'].value_counts())\n",
        "#dados_filtered = resultados[resultados['relevant'] != 'False']\n",
        "#dados_filtered.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(resultados['document title'].head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(resultados[resultados['relevant_gemma3'] == True])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "positivos = resultados[resultados[\"relevant\"] == True].copy()\n",
        "print(positivos.head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Critrérios de exclusão"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
