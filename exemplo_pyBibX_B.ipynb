{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO7ZYzeden9c"
      },
      "outputs": [],
      "source": [
        "# Restart the session afther this cell to avoid Google Colab errors\n",
        "!pip install --upgrade --force-reinstall numpy==1.26.4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF29icAhigEy"
      },
      "outputs": [],
      "source": [
        "!pip install pybibx\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGlz58a_iQhX"
      },
      "outputs": [],
      "source": [
        "# Dowload .bib file\n",
        "#!wget https://github.com/Valdecy/pyBibX/raw/main/assets/bibs/scopus.bib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUDAb0D_aV53"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import textwrap\n",
        "\n",
        "from pybibx.base import pbx_probe\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U94aI4NIjY-k"
      },
      "outputs": [],
      "source": [
        "# Load .bib\n",
        "# Arguments: file_bib = 'filename.bib'; db = 'scopus', 'wos', 'pubmed'; del_duplicated = True, False\n",
        "file_name = 'dados/scopus.bib'\n",
        "database  = 'scopus'\n",
        "bibfile   = pbx_probe(file_bib = file_name, db = database, del_duplicated = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bja7UImUpuK"
      },
      "outputs": [],
      "source": [
        "# Health Analysis\n",
        "health = bibfile.health_bib()\n",
        "\n",
        "# Check Health\n",
        "health"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDwUW-yofYlI"
      },
      "outputs": [],
      "source": [
        "print(bibfile.data['abstract'].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjFFdFz3gjX4"
      },
      "outputs": [],
      "source": [
        "!pip install pybtex\n",
        "!pip install bibtexparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXTHbzlJhHcg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import bibtexparser\n",
        "from bibtexparser.bwriter import BibTexWriter\n",
        "from bibtexparser.bibdatabase import BibDatabase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import traceback\n",
        "import random\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from functools import partial\n",
        "import pandas as pd\n",
        "import ollama\n",
        "\n",
        "# === CONFIGURAÇÕES ===\n",
        "MAX_REQUESTS_PER_MINUTE = 600\n",
        "SECONDS_BETWEEN_REQUESTS = 60 / MAX_REQUESTS_PER_MINUTE\n",
        "BATCH_SIZE = 1\n",
        "WORKERS = min(12, cpu_count())\n",
        "MODEL = \"gemma3:27b\"  # Ajuste para o modelo local desejado\n",
        "TEMPERATURE = 0.2\n",
        "RESULT_CSV_PATH = \"temp_files/resultados_parciais.csv\"\n",
        "LOG_PATH = \"temp_files/log_execucao.txt\"\n",
        "QUERY = (\n",
        "    \"Does this abstract discuss artificial intelligence in feedback for learning management systems on education?\"\n",
        ")\n",
        "\n",
        "# === UTILITÁRIOS ===\n",
        "def log(text: str) -> None:\n",
        "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"{timestamp} - {text}\\n\")\n",
        "    print(f\"{timestamp} - {text}\")\n",
        "\n",
        "\n",
        "def chunk_dataframe(df, batch_size: int):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i : i + batch_size], i\n",
        "\n",
        "# === FUNÇÃO PARA CHAMAR O LLM LOCAL ===\n",
        "def call_local_llm(messages, model: str):\n",
        "    \"\"\"\n",
        "    Envia a lista de mensagens para o modelo local via Ollama e retorna o texto de resposta.\n",
        "    \"\"\"\n",
        "    response = ollama.chat(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        options={\"temperature\": TEMPERATURE},\n",
        "        stream=False\n",
        "    )\n",
        "    # O conteúdo da resposta geralmente está em response.message.content\n",
        "    return response.message.content.strip()\n",
        "\n",
        "# === FUNÇÃO DE PROCESSAMENTO DE LOTE COM RETRIES ===\n",
        "def process_batch_with_retry(\n",
        "    batch_df, global_index, query=QUERY, model=MODEL, retry_limit=5\n",
        "):\n",
        "    retry_count = 0\n",
        "    delay = SECONDS_BETWEEN_REQUESTS + random.uniform(0, 5)\n",
        "\n",
        "    while retry_count < retry_limit:\n",
        "        try:\n",
        "            time.sleep(delay)\n",
        "            # Monta as mensagens para o LLM\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": (\n",
        "                    \"You are a research assistant who helps analyze scientific articles.\"\n",
        "                )}\n",
        "            ]\n",
        "            prompt = f\"{query}\\n\\nRestrict yourself to answering the question with exclusively 'yes' or 'no'.\\n\\n\"\n",
        "\n",
        "            # Adiciona cada abstract ao prompt\n",
        "            for i, row in batch_df.iterrows():\n",
        "                prompt += f\"Abstract {i + 1}:\\n{row['abstract']}\\n\\n\"\n",
        "\n",
        "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "            # Chama o LLM local\n",
        "            content = call_local_llm(messages, model=model)\n",
        "            answers = content.splitlines()\n",
        "\n",
        "            # Monta os resultados\n",
        "            results = []\n",
        "            for answer, (_, row) in zip(answers, batch_df.iterrows()):\n",
        "                result = row.to_dict()\n",
        "                clean = answer.strip().lower()\n",
        "                result[\"relevant\"] = clean == \"yes\"\n",
        "                result[\"tokens_used\"] = None  # Não disponível no Ollama local\n",
        "                results.append(result)\n",
        "\n",
        "            log(f\"Lote {global_index} processado com sucesso.\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            retry_count += 1\n",
        "            wait_time = 2 ** retry_count + random.uniform(0, 1)\n",
        "            log(f\"[ERRO] Lote {global_index}, tentativa {retry_count}: {e}\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "    log(f\"[FALHA] Lote {global_index} excedeu o limite de tentativas.\")\n",
        "    return []\n",
        "\n",
        "# === WRAPPER PARA MULTIPROCESSING ===\n",
        "def process_args_wrapper(args, query, model):\n",
        "    return process_batch_with_retry(*args, query=query, model=model)\n",
        "\n",
        "# === FUNÇÃO PRINCIPAL ===\n",
        "def analyze_abstracts_parallel(df: pd.DataFrame, query=QUERY, model=MODEL,\n",
        "                                batch_size=BATCH_SIZE, workers=WORKERS):\n",
        "    # Retoma de arquivo se existir\n",
        "    if os.path.exists(RESULT_CSV_PATH):\n",
        "        acumulado = pd.read_csv(RESULT_CSV_PATH)\n",
        "        start = len(acumulado)\n",
        "        log(f\"Retomando a partir do índice {start}\")\n",
        "    else:\n",
        "        acumulado = pd.DataFrame()\n",
        "        start = 0\n",
        "\n",
        "    to_process = df.iloc[start:].reset_index(drop=True)\n",
        "    batches = [(batch, idx + start) for batch, idx in chunk_dataframe(to_process, batch_size)]\n",
        "\n",
        "    log(f\"Iniciando {len(batches)} lotes com {workers} workers\")\n",
        "\n",
        "    with Pool(processes=workers) as pool:\n",
        "        processor = partial(process_args_wrapper, query=query, model=model)\n",
        "        for outcome in pool.imap_unordered(processor, batches):\n",
        "            if outcome:\n",
        "                df_part = pd.DataFrame(outcome)\n",
        "                acumulado = pd.concat([acumulado, df_part], ignore_index=True)\n",
        "                acumulado.to_csv(RESULT_CSV_PATH, index=False)\n",
        "\n",
        "    log(\"Processamento completo.\")\n",
        "    return acumulado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_filter_bases(directory: str) -> pd.DataFrame:\n",
        "    log(f\"Carregando bases de '{directory}'\")\n",
        "    dfs = []\n",
        "    for fname in sorted(os.listdir(directory)):\n",
        "        if fname.lower().endswith('.csv'):\n",
        "            path = os.path.join(directory, fname)\n",
        "            try:\n",
        "                df = pd.read_csv(path)\n",
        "                log(f\"{fname}: {len(df)} registros\")\n",
        "                dfs.append(df)\n",
        "            except Exception as e:\n",
        "                log(f\"Erro lendo {fname}: {e}\")\n",
        "    if not dfs:\n",
        "        return pd.DataFrame()\n",
        "    df = pd.concat(dfs, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dados = bibfile.data\n",
        "dados = dados\n",
        "\n",
        "df_ieee = load_and_filter_bases(\"dados\")\n",
        "colunas_desejadas_ieee = ['Document Title', 'Abstract', 'Author Affiliations', 'Authors', 'DOI', 'ISBNs',\n",
        "                             'ISSN', 'Publication Title', 'Publication Year']\n",
        "df_ieee = df_ieee[colunas_desejadas_ieee].copy()\n",
        "print(df_ieee.columns)\n",
        "df_scopus = dados.rename(columns={\n",
        "    'title': 'Document Title',\n",
        "    'abstract': 'Abstract',\n",
        "    'abbrev_source_title': 'Publication Title',\n",
        "    'affiliation': 'Author Affiliations',\n",
        "    'author': 'Authors',\n",
        "    'doi': 'DOI',\n",
        "    'isbn': 'ISBNs',\n",
        "    'issn': 'ISSN',\n",
        "    'journal': 'Publication Title',\n",
        "    'references': 'References',\n",
        "    'url': 'URL',\n",
        "    'year': 'Publication Year'\n",
        "}, inplace=True)\n",
        "\n",
        "colunas_desejadas_scopus = ['title', 'abstract', 'abbrev_source_title', \n",
        "                            'affiliation', 'author', 'document_type', 'doi', 'isbn',\n",
        "                             'issn', 'journal', 'references', 'url', 'year']\n",
        "df_scopus = dados[colunas_desejadas_scopus].copy()\n",
        "# Certifique-se de que o DataFrame `dados` contém pelo menos as colunas 'abstract' e outras desejadas\n",
        "#resultados = analyze_abstracts_parallel(dados, query=query_global, model=model, batch_size=5, workers=8)\n",
        "\n",
        "dados = pd.concat([df_ieee, df_scopus], ignore_index=True)\n",
        "print(dados.shape)\n",
        "dados = dados.dropna(subset=['abstract'])\n",
        "dados = dados.drop_duplicates(subset=['abstract'])\n",
        "dados = dados.reset_index(drop=True)\n",
        "log('Após remoção de duplicados %s', dados.shape)\n",
        "\n",
        "resultados = analyze_abstracts_parallel(\n",
        "    df=dados,\n",
        "    query=QUERY,\n",
        "    model=MODEL,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    workers=WORKERS\n",
        ")\n",
        "\n",
        "# Salvar CSV final (opcional)\n",
        "resultados.to_csv(\"temp_files/resultados_finais.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT68x1YipfZg"
      },
      "outputs": [],
      "source": [
        "print(resultados.columns)\n",
        "print(resultados['relevant'].value_counts())\n",
        "#dados_filtered = resultados[resultados['relevant'] != 'False']\n",
        "#dados_filtered.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "positivos = resultados[resultados[\"relevant\"] == True].copy()\n",
        "#print(positivos.head(5))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
